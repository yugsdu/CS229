{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kullback-Leibler (KL) divergence "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "KL(P \\parallel Q)\n",
    "&= KL(P(X) \\parallel Q(X)) \\\\\n",
    "&= \\sum_{x} P(x) \\log \\frac{P(x)}{Q(x)}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (a) Nonnegativity "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prove the following:\n",
    "\n",
    "$$ \\forall P,Q \\;\\;KL(P\\parallel Q) \\ge 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this inequality can be proven as following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "KL(P \\parallel Q)\n",
    "&= \\sum_{x} P(x) \\log \\frac{P(x)}{Q(x)}\\\\\n",
    "&= -\\sum_{x} P(x) \\log \\frac{Q(x)}{P(x)}\\\\\n",
    "&= \\sum_{x} P(x) (-\\log \\frac{Q(x)}{P(x)})\\\\\n",
    "&= E[-\\log \\frac {Q(x)}{P(x)}]\\\\\n",
    "&\\ge -\\log E[\\frac {Q(x)}{P(x)}]\\\\\n",
    "&= -\\log \\sum_{x} P(x)\\frac {Q(x)}{P(x)}\\\\\n",
    "&= -\\log 1\\\\\n",
    "&= 0\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "note that the first inequality comes from the Jensen's inequality since $-\\log x$ is a convex function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (b) Chain rule for KL divergence "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the KL divergence between 2 conditional distributions $P(X|Y)$, $Q(X|Y)$ is defined as follows:\n",
    "\n",
    "\\begin{align*}\n",
    "KL(P(X|Y) \\parallel Q(X|Y))\n",
    "&= \\sum_{y} P(y)\\bigg(\\sum_{x} P(x|y)\\log \\frac{P(x|y)}{Q(xy)}\\bigg)\\\\\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prove the following chain rule for KL divergence:\n",
    "\n",
    "$$\n",
    "KL(P(X,Y) \\parallel Q(X,Y))= KL(P(X) \\parallel Q(X)) + KL(P(Y|X)\\parallel Q(Y|X))\\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can prove this step by step:\n",
    "\n",
    "\\begin{align*}\n",
    "KL(P(X) \\parallel Q(X))+KL(P(Y|X) \\parallel Q(Y|X))\n",
    "&= \\sum_{x} P(x) \\log \\frac{P(x)}{Q(x)}+\\sum_{x} P(x)\\bigg(\\sum_{y}P(y|x)\\log \\frac{P(y|x)}{Q(y|x)}\\bigg)\\\\\n",
    "&= \\sum_{x} P(x) \\bigg(\\log \\frac{P(x)}{Q(x)}+\\sum_{y}P(y|x)\\log \\frac{P(y|x)}{Q(y|x)}\\bigg)\\\\\n",
    "&= \\sum_{x} P(x) \\bigg(\\sum_{y}P(y|x)\\bigg(\\log \\frac{P(x)}{Q(x)}+\\log \\frac{P(y|x)}{Q(y|x)}\\bigg)\\bigg)\\\\\n",
    "&= \\sum_{x} P(x) \\bigg(\\sum_{y}P(y|x)\\bigg(\\log \\frac{P(x)P(y|x)}{Q(x)Q(y|x)}\\bigg)\\bigg)\\\\\n",
    "&= \\sum_{x} P(x) \\bigg(\\sum_{y}P(y|x)\\bigg(\\log \\frac{P(x,y)}{Q(x,y)}\\bigg)\\bigg)\\\\\n",
    "&= \\sum_{x,y} P(x,y) \\log \\frac{P(x,y)}{Q(x,y)}\\\\\n",
    "\\end{align*}\n",
    "\n",
    "the third equality comes from the fact that $\\sum_{y}P(y|x)=1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (c) KL and maximum likelihood "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "given $$\\hat P(x) = \\frac {1}{m}\\sum_{i=1}^{m}1({x}^{(i)}=x)$$\n",
    "\n",
    "\n",
    "prove $$\\arg\\!\\min_{\\theta}KL(\\hat P \\parallel P_{\\theta}) = \\arg\\!\\max_{\\theta}\\sum_{i=1}^{m}\\log P_{\\theta}(x^{i})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can prove this equation step by step:\n",
    "\n",
    "\\begin{align*}\n",
    "KL(\\hat P\\parallel P_{\\theta})\n",
    "&=\\sum_{x}P(x)\\log \\frac {\\hat P(x)}{P_{\\theta}(x)}\\\\\n",
    "&=\\sum_{x} \\frac {1}{m}\\sum_{i=1}^{m}1({x}^{(i)}=x) \\log \\frac {\\frac {1}{m}\\sum_{i=1}^{m}1({x}^{(i)}=x)}{P_{\\theta}(x)}\\\\\n",
    "&=-\\sum_{x} \\frac {1}{m}\\sum_{i=1}^{m}1({x}^{(i)}=x) \\log \\frac {P_{\\theta}(x)}{\\frac {1}{m}\\sum_{i=1}^{m}1({x}^{(i)}=x)}\\\\\n",
    "&=-\\frac {1}{m}\\sum_{i=1}^{m} \\log \\frac {P_{\\theta}(x)}{\\frac {1}{m}\\sum_{i=1}^{m}}\\\\\n",
    "&=-\\frac {1}{m}\\sum_{i=1}^{m} \\log P_{\\theta}(x)\\\\\n",
    "&= -\\frac {1}{m}log-likelihood\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the fourth equality is so strange, and I am not sure whether it is true. However I haven't thought out a better solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
