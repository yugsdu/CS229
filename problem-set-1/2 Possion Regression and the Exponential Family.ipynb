{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a) Show that the Poisson distribution is in the exponential family."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Possion distribuiton parameterized by $\\lambda$:\n",
    "\n",
    "\\begin{align*}\n",
    "p(y; \\lambda)\n",
    "&= \\frac{e^{-\\lambda} \\lambda ^y}{y!} \\\\\n",
    "&= \\frac{1}{y!}\\exp \\big(y \\ln{\\lambda} - \\lambda\\big) \\\\\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "since the formula of exponential family is:\n",
    "$p(y;\\eta) = b(y) \\exp\\big(\\eta T(y) - a(\\eta)\\big) $,so:\n",
    "\n",
    "\\begin{align*}\n",
    "b(y) &= \\frac{1}{y!} \\\\\n",
    "\\eta &= \\ln \\lambda \\\\\n",
    "T(y) &= y \\\\\n",
    "a(\\eta) &= \\lambda = e^{\\eta} \\\\\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b) What is the canonical response function for the family?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "g(\\eta)\n",
    "&= E[T(y); \\eta] \\\\\n",
    "&= E[y; \\eta] \\\\\n",
    "&= \\lambda \\\\\n",
    "&= e^{\\eta} \\\\\n",
    "&= e^{\\theta^T x} \\\\\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (c) Derivate the stochastic gradient ascent rule for learning using a GLM model with Poisson responses $y$ and the canonical response function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!! Following answer comes from “zyxue/stanford-cs229”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since its log-likelihood function is:\n",
    "\n",
    "\\begin{align*}\n",
    "l(\\theta) &= \\log p(y; \\lambda) \\\\\n",
    "          &= \\log \\frac{1}{y!} + y \\ln \\lambda - \\lambda \\\\\n",
    "          &= \\log \\frac{1}{y!} + y \\eta - e^{\\eta} \\\\\n",
    "          &= \\log \\frac{1}{y!} + y \\theta^T x - e^{\\theta^T x}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can derivate its gradient:\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial l(\\theta)}{\\partial \\theta_j}\n",
    "&= y x_j - e^{\\theta^T x} x_j \\\\\n",
    "&= \\big(y - e^{\\theta^T x}\\big) x_j \\\\\n",
    "&= \\big(y - e^{\\eta}\\big) x_j \\\\\n",
    "&= (y - \\lambda) x_j\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, we can get the stochastic gradient ascent rule:\n",
    "    \n",
    "\\begin{align*}\n",
    "\\theta_j &:= \\theta_j + \\alpha \\big(y^{(i)} - e^{\\theta^T x^{(i)}}\\big) x_j^{(i)}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (d) Show that the stochastic gradient ascent on the log-likelihood $\\log p(y|X;\\theta)$ results in the update rule $\\theta_j := \\theta_j - \\alpha(h(x)-y)x_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the fact that $T(y) = y$, GLM becomes\n",
    "\n",
    "$$p(y;\\eta) = b(y) \\exp\\big(\\eta y - a(\\eta)\\big) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "its log-likelihood function is:\n",
    "    \n",
    "$$\\log p(y;\\eta) = \\log b(y) + \\eta y - a(\\eta)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "calculate the gradient:\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial \\log p(y;\\eta)}{\\partial \\theta_j} \n",
    "&= 0 + y \\frac{\\partial \\eta}{\\theta_j} - \\frac{\\partial a(\\eta)}{\\partial \\eta} \\frac{\\partial \\eta}{\\theta_j} \\\\\n",
    "&= y x_j  - \\frac{\\partial a(\\eta)}{\\partial \\eta} x_j \\\\\n",
    "&= \\Big(y - \\frac{\\partial a(\\eta)}{\\partial \\eta}\\Big) x_j\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that there is the fact that $$ \\frac{\\partial a(\\eta)}{\\partial \\eta} = h(x)$$ (complex derivatio processes)!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perhaps, this is the most difficult process in the probelm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we would have:\n",
    "    \n",
    "$$ \\frac{\\partial \\log p(y;\\eta)}{\\partial \\theta_j}  = \\big(y - h(x)\\big) x_j$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in the end, we could get the stochastic gradient rule:\n",
    "\n",
    "\\begin{align*}\n",
    "\\theta_j \n",
    "&:= \\theta_j + \\alpha \\big(y^{(i)} - h(x^{(i)} \\big)x_j^{(i)} \\\\\n",
    "&:= \\theta_j - \\alpha \\big(h(x^{(i)} - y^{(i)} \\big)x_j^{(i)}\n",
    "\\end{align*}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
